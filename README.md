## ğŸš€ Agentic Load Balancer using Deep Reinforcement Learning

An intelligent, reinforcement learningâ€“based cloud load balancer that dynamically allocates workloads across servers and performs adaptive autoscaling to optimize latency, load balance, and SLA compliance.

This project compares a Deep Q-Network (DQN) agent against traditional load balancing heuristics under stochastic cloud workloads.

---

## ğŸ“Œ Overview

Modern cloud systems face dynamic and unpredictable workloads. Traditional load balancing strategies (e.g., Round Robin, Least Connections) do not adapt optimally to fluctuating demand and SLA constraints.

This project implements an Agentic Load Balancer using Deep Reinforcement Learning that:

- Learns optimal routing policies
- Performs intelligent autoscaling
- Minimizes latency and SLA violations
- Maintains balanced CPU utilization
- Adapts to stochastic request patterns

---

## ğŸ§  Key Features

- âœ… Deep Q-Network (DQN) based load balancing
- âœ… Configurable autoscaling (enable/disable)
- âœ… Poisson-distributed stochastic workload simulation
- âœ… GPU optional (automatic CPU fallback)
- âœ… Replay buffer with experience replay
- âœ… Target network stabilization
- âœ… Baseline comparisons:
  - Random Allocation
  - Round Robin
  - Least Connections
- âœ… Convergence analysis & reward visualization

---

## ğŸ—ï¸ Project Architecture
```
CC Project
â”‚
â”œâ”€â”€ cloud_env_simulator.py            # Cloud environment simulation
â”œâ”€â”€ configuration.py                  # All configurable hyperparameters
â”œâ”€â”€ dqn_agent.py                      # DQN model + replay buffer
â”œâ”€â”€ dqn_agent_trainer.py              # Training loop
â”œâ”€â”€ baselines_vs_dqn_evaluator.ipynb  # Evaluation & plotting
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
```

---

## âš™ï¸ How It Works
### Environment (cloud_env_simulator.py)

Simulates:
  - Server CPU utilization
  - Request queues
  - Poisson-distributed incoming workload
  - Processing rates
  - SLA violation penalties
  - Autoscaling actions
  - State Representation

Includes:
  - CPU utilization per server
  - Queue length per server
  - Average latency
  - Number of active servers
  - Load variance
  - Action Space

If autoscaling enabled:

  - 0 â†’ MAX_SERVER_COUNT - 1   : Route to server
  - MAX_SERVER_COUNT           : Scale up
  - MAX_SERVER_COUNT + 1       : Scale down

Otherwise:

  - 0 â†’ MAX_SERVER_COUNT - 1   : Route to server

---

## ğŸ¯ Reward Function

The agent minimizes:
  - Average latency
  - SLA violations
  - Load imbalance
  - Excessive server usage
  - Reward= âˆ’ [Latency + SLA_Penalty + LoadVariance + ScalingCost]

This encourages stable, balanced, low-latency operation.

---

## ğŸ–¥ï¸ Installation

### 1ï¸âƒ£ Clone Repository
git clone https://github.com/HaZaRdOuSDeVeLoPeR/CC-Project.git  
cd CC-Project

### 2ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate

### 3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

### 4ï¸âƒ£ (Optional) Install CUDA-enabled PyTorch for GPU

If you have an NVIDIA GPU:
  - pip install torch --index-url https://download.pytorch.org/whl/cu121

---

### baselines_vs_dqn_evaluator.ipynb
  - This notebook evaluates all baselines
  - Compares performance
  - Plots reward distributions
  - Demonstrates convergence

## ğŸ”§Configuration Options

All configurable parameters are centralized in: configuration.py  
Including:
  - Server counts
  - Processing rate
  - SLA thresholds
  - Incoming workload parameters
  - DQN hyperparameters
  - Training episode count
  - Device selection

This makes experimentation and ablation studies easy.

---

## ğŸ§ª Research Observations

  - DQN converges to a steady-state control strategy.
  - Evaluation with Îµ = 0 yields deterministic performance.
  - Autoscaling significantly improves stability under stochastic load.
  - Learned policy absorbs workload randomness better than heuristics.
  - Replay buffer and target network stabilize training.

---

## ğŸ“Œ Future Improvements

  - Double DQN implementation
  - Prioritized Experience Replay
  - Parallelized environments
  - Multi-agent distributed control
  - Carbon-aware scheduling integration
  - Serverless simulation extension

---

## ğŸ“š Technologies Used

  - Python 3.10
  - PyTorch
  - NumPy
  - Matplotlib
  - Reinforcement Learning (DQN)

---

## ğŸ‘¨â€ğŸ’» Author

Aditya Vimal  
B.Tech CSE  
NIT Warangal
